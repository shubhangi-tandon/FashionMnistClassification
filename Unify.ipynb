{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras import applications\n",
    "# Plot images\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "# plot 4 images as gray scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2TRe1/8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e09b990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing data\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "x_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train shape:', (60000, 28, 28, 1))\n",
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "num_classes = len(set(y_train))\n",
    "print num_classes\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a callback that records the activation output for the fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print model.summary()\n",
    "import keras.backend as K\n",
    "class ActivationOutputHistory(keras.callbacks.Callback):\n",
    "  \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.activation4 = []\n",
    "        self.activation5 = []\n",
    "    \n",
    "    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print (\"in activation output callback \")\n",
    "        get_fourth_layer_output= K.function([self.model.layers[0].input, K.learning_phase()],\n",
    "                                  [self.model.layers[4].output])\n",
    "        get_fifth_layer_output= K.function([self.model.layers[0].input, K.learning_phase()],\n",
    "                                  [self.model.layers[5].output])\n",
    "        \n",
    "        self.losses.append(logs.get('loss'))\n",
    "        #layer_output = get_3rd_layer_output([X, 1])[0]\n",
    "        print (\"getting output for 4th layer \")\n",
    "        self.activation4.append(get_fourth_layer_output([x_train[:500], 1])[0])\n",
    "        self.activation5.append(get_fifth_layer_output([x_train[:500], 1])[0])\n",
    "         print (\"getting output for 5th layer \")\n",
    "        self.activation4[-1] += get_fourth_layer_output([x_train[500:], 1])[0]\n",
    "        self.activation5[-1] += get_fifth_layer_output([x_train[500:], 1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/2\n",
      " 896/1000 [=========================>....] - ETA: 0s - loss: 1.5916 - acc: 0.4989"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (500,9216) (59500,9216) (500,9216) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d41e747b9bbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           validation_data=(x_test[:200], y_test[:200]), callbacks = [activation_history])\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# #print ret.history()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shubhi/anaconda/envs/py27/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/shubhi/anaconda/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/shubhi/anaconda/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shubhi/anaconda/envs/py27/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-98a5e900f96b>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_fourth_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_fifth_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_fourth_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_fifth_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (500,9216) (59500,9216) (500,9216) "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 2\n",
    "activation_history = ActivationOutputHistory()\n",
    "\n",
    "ret = model.fit(x_train[:1000], y_train[:1000],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test[:200], y_test[:200]), callbacks = [activation_history])\n",
    "         \n",
    "# #print ret.history()\n",
    "\n",
    "# ret = model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test), callbacks = [activation_history])\n",
    "         \n",
    "#print ret.history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s 1ms/step\n",
      "('Test loss:', 1.6634386386871338)\n",
      "('Test accuracy:', 0.49859999999999999)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [0.21199999713897705, 0.37600000143051149], 'loss': [2.206793514251709, 1.9345671072006225], 'val_acc': [0.46000000834465027, 0.5], 'val_loss': [1.9755597114562988, 1.6749768257141113]}\n",
      "2\n",
      "(500, 9216)\n",
      "2\n",
      "(500, 128)\n"
     ]
    }
   ],
   "source": [
    "print ret.history\n",
    "print len(activation_history.activation4)\n",
    "print activation_history.activation4[0].shape\n",
    "print len(activation_history.activation5)\n",
    "print activation_history.activation5[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"this function will take the output of hidden layers and bin them into 30 bins (by default) from -1 to 1 and return the means and probability of each bin \")\n",
    "def bin_hidden_layers(T, num_bins = 30, beg = -1, end =1 ):\n",
    "    import numpy as np\n",
    "    data = np.array(T)\n",
    "    bins = numpy.linspace(beg, end, num_bins)\n",
    "    digitized = numpy.digitize(data, bins)\n",
    "    print (digitized)\n",
    "    bin_means = [data[digitized == i].mean() for i in range(1, len(bins) +1)]\n",
    "    print (bin_means)\n",
    "    \n",
    "    binned_data = digitized -1\n",
    "\n",
    "    from collections import Counter\n",
    "    counter = Counter(binned_data)\n",
    "\n",
    "    binned_mean = []\n",
    "    binned_probability = []\n",
    "    for class1 in counter :\n",
    "        prob = counter[class1]/ len(binned_data)\n",
    "        bin_mean = bin_means[class1]\n",
    "#         print (class1 )\n",
    "#         print (prob)\n",
    "#         print (bin_mean)\n",
    "        binned_mean.append(bin_mean)\n",
    "        binned_probability.append(prob)\n",
    "\n",
    "    \n",
    "    return (binned_mean , binned_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"test function for binning \")\n",
    "import numpy \n",
    "T  = [-1, -0.7, -0.4, -0.3, -0.2,-1, -0.7, -0.4, -0.3, -0.2,-1, -0.7, -0.4, \n",
    "      -0.3, -0.2,-1, -0.7, -0.4, -0.3, -0.2, 1, 0, 0.4, 0.3, 0.2, 0.1, 0.5, 0.55, 0.56 ]\n",
    "T = numpy.array(T)\n",
    "print (T)\n",
    "binned_mean , binned_probability = bin_hidden_layers(T, num_bins=4)\n",
    "print (binned_mean)\n",
    "print (binned_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- We need to find :\n",
    "- I(X;T), I(T;Y)\n",
    "- I(X;T) = H(X) - H(X|T)\n",
    "- I(T;Y) = H(T) - H(T|Y)\n",
    "\n",
    "- H(X) =  - Sum(p(x) log p(x))\n",
    "\n",
    "- We need P(X), P(T), P(X|T), P(T|Y)\n",
    "\n",
    "- Calculate P(x) -> we can take each datapoint to be equally distributed , so the 1/num_of_samples\n",
    "- Calculate P(y) -> we can calculate by just taking distribution of sample labels using Counter \n",
    "\n",
    "- get value of t for every neuron , bin this data into (30?) intervals , get a distribution of P(T) ( also P(T,x ) = ?))\n",
    "- P(Ti; Y ) = Sum( P(x, Y )P(Ti|x) )\n",
    "\n",
    "\n",
    "- Train over 100 epochs\n",
    "- Get value for hidden layers for each data point\n",
    "\n",
    "- We have distributions of X and Y , need the distribution of T.  We can do this by generating the values of T after every epoch (possible with tf, see if Keras has too)  \n",
    "- scikit learn method : http://scikit-learn.org/stable/modules/clustering.html#mutual-info-score \n",
    "-  We have to get T , look at call back in Keras \n",
    "\n",
    "-  Done Bin T as given in paper into 30? (maybe 40 for ease of calculation)  and get a distribution of T after every epoch \n",
    "\n",
    "- So for every X  [ where X is a  28 *28 *3 vector ] we will have multiple Ts and one Y  \n",
    "\n",
    "- Solution 1\n",
    "- get distributions for T, X and Y and calculate mutual info score using sklearn.mutual_info_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}